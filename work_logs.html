<h5 align=right>2016 mar 5</h5>
<h2>Reproduce Hagan Example</h2>
Creating this in simple_backprop.py, using Hagan example on p. 11-14 to 11-17.
<p>
Reproduced Hagan numbers.
<p>
Refactored and created test case for simple two layer backprop using Hagan text.<br>
Testing like:
<pre>
(nn_python2)
[ytsboe@ytsboe-Latitude-E7250 ~/github/nn (master)]
$ python ./test_simple_backprop.py
</pre>
<h2>Prepare data</h2>
Fetched "raw" data from Digital Ocean site, put here: <tt>/home/ytsboe/data/boats/raw_data</tt>
<p>
Script to convert raw boat data to computer reabable:
<tt>/home/ytsboe/github/nn/boats/raw_to_computer_readable.py</tt>
<p>
Here "computer readable" meamns ready for making plots etc., but before "cleaning" (removing outliers etc.)
<p>
Tested fetching of data:
<pre>
Averages:
length_over_all_meters: 11.5057620164
width_meters: 3.61302763081
build_year: 1993.74452986
draft_meters: 1.75096601073
displaces_kgs: 9198.89443652
ballast_kgs: 3111.76237624
engine_build_year: 1998.50480769

Sums:
  None: 1005.0
  jeanneau: 122.0
  etap: 37.0
  contest: 37.0
  beneteau: 98.0
  hanse: 95.0
  hallberg-rassy: 73.0
  dehler: 60.0
  dufour: 60.0
  bavaria: 147.0
</pre>
All fine.

<h2>Analyse/clean input data</h2>
We'll have to add cuts to the data for sanitation.<br>
For example:
<ul>
<li>Remove very long boat (93m): <tt>/home/ytsboe/data/boats/raw_data/santarelli-modulo-93-89194.dat</tt></li>
<li>Remove very broad boat (32m): <tt>/home/ytsboe/data/boats/raw_data/j-boats-35-91943.dat</tt></li>
</ul>
<p>
The cuts will be:
<p>
<table border=1>
  <tr><td>
      asking price
    </td><td>
      gt 1 and lt 1000000
  </td></tr>
  <tr><td>
      length
    </td><td>
      lt 30m
  </td></tr>
  <tr><td>
      width
    </td><td>
      lt 8m
  </td></tr>
  <tr><td>
      build year
    </td><td>
      gt 1960
  </td></tr>
  <tr><td>
      draft
    </td><td>
      lt 3.3
  </td></tr>
  <tr><td>
      displaces
    </td><td>
      lt 50000
  </td></tr>
  <tr><td>
      ballast
    </td><td>
      lt 8000
  </td></tr>
</table>
Stored the pdf with the plots here: <tt>/home/ytsboe/github/nn/boats/20160305_histograms_raw_1.pdf</tt>
<p>
Applied these cuts, plots have now better range: <tt>/home/ytsboe/github/nn/boats/20160305_histograms_cleaned.pdf</tt>
<p>
<h5 align=right>2016 mar 6</h5>
Normalised the data, plots here:<br>
<pre>
[ytsboe@ytsboe-Latitude-E7250 ~/github/nn/boats (master)]
$ cp feature_distributions.pdf 20160306_histograms_normalised.pdf
</pre>
Price correlates with length, with, draft and build year.<br>
Little extra information expected from balast, displacement and engine build year.<br>
<p>
Plotted features scatter plots:
<pre>
[ytsboe@ytsboe-Latitude-E7250 ~/github/nn/boats (master)]
$ cp feature_scatter_plots.pdf 20160306_feature_scatter_plots.pdf 
</pre>
We see the expected large correlation of length with both with and draft.<br>
Same for ballast and displacement, where also the subsitution of averages
spoils the correlation.<br>
<p>
In short, we expect the length and build year as the only significant features.<br>
<p>
Plotted distributions of asking prices for all builders and compared the averages:
<pre>
$ python plot_input_data.py
Asking prices (26.0, avg=-0.90) hist for builder: compromis
Asking prices (810.0, avg=-0.79) hist for builder: None
Asking prices (121.0, avg=-0.81) hist for builder: jeanneau
Asking prices (37.0, avg=-0.88) hist for builder: etap
Asking prices (34.0, avg=-0.76) hist for builder: contest
Asking prices (24.0, avg=-0.87) hist for builder: victoire
Asking prices (28.0, avg=-0.84) hist for builder: elan
Asking prices (92.0, avg=-0.82) hist for builder: beneteau
Asking prices (68.0, avg=-0.73) hist for builder: hallberg-rassy
Asking prices (86.0, avg=-0.67) hist for builder: hanse
Asking prices (60.0, avg=-0.83) hist for builder: dehler
Asking prices (60.0, avg=-0.85) hist for builder: dufour
Asking prices (146.0, avg=-0.83) hist for builder: bavaria
Average average price = -0.812876360256

[ytsboe@ytsboe-Latitude-E7250 ~/github/nn/boats (master)]
$ cp asking_price_builder_names.pdf 20160306_asking_price_builder_names.pdf
</pre>
Hallberg Rassy and Hanse boats are significantly more expensive than average.<br>
The Compromis boats are significantly less expensive.
<p>
Managed to apply nn to the boats.<br>
Found various minima, lowest about 0.03 for mse.<br>
<p>
Below: error is plain error, not mse.
<pre>
$ python ./nn_simple_boats.py
S1 = 5 alpha = 1e-05
Iteration: 10 error train: [[ 0.75295619]] error test: [[ 0.77302896]]  
Iteration: 14 error train: [[ 0.75286913]] error test: [[ 0.77293925]]
...
Iteration: 4468866 error train: [[ 0.19906822]] error test: [[ 0.18165948]]
Iteration: 6351925 error train: [[ 0.19906822]] error test: [[ 0.18165948]]
Iteration: 9028455 error train: [[ 0.19906822]] error test: [[ 0.18165948]]
Saving mses plot to mses.png

W1    = [[-0.24212475  0.02694244  0.02821757 -0.08954417 -0.06884544 -0.22029359
  -0.17824722  0.12016396  0.07804248 -0.12401254 -0.23812842  0.0067572
  -0.07502254  0.09752009 -0.14860355 -0.22934456  0.04440873 -0.19319109
  -0.21739163  0.04663912]
 [-0.19111259 -0.04972988  0.2766238  -0.20549939 -0.23706564 -0.00905296
  -0.07868101 -0.01806229  0.11763035  0.21292985 -0.06923144  0.03853001
  -0.18160576  0.15787528 -0.03442326 -0.14595591  0.06767789  0.15274417
  -0.23525094  0.01755163]
 [ 0.08318391 -0.18704918  0.02575674 -0.21834164 -0.02657485 -0.03083301
  -0.01879369 -0.13354374 -0.07215475  0.10100178  0.01558534 -0.20290369
  -0.02158654  0.10643996 -0.17490313  0.01301879  0.15909314  0.23171723
  -0.0187789   0.05825458]
 [ 0.17224199  0.25418321  0.17877259  0.00282     0.10864183  0.14682755
  -0.18455865 -0.03242237  0.10119335  0.01979395 -0.16845622  0.10966697
   0.0434327   0.12205877  0.22940673  0.21840603 -0.13924256 -0.04318723
  -0.24359756  0.07975306]
 [ 0.25373977  0.09699836 -0.25198931  0.04910205 -0.12344794  0.05795805
   0.00623658 -0.03152442 -0.01956241 -0.08682047  0.08043537 -0.14919747
  -0.05816138 -0.18726213  0.08601861  0.12166136 -0.0105374  -0.035562
  -0.1693814   0.03321968]]
b1vec = [[-0.03134725]
 [ 0.15939111]
 [-0.18518457]
 [-0.00446784]
 [-0.11154539]]
W2    = [[-0.23789441 -0.41403788  0.09546187  0.23704932  0.22342746]]
b2vec = [[-0.39715867]]
Average diff = -0.150578919606 sd = 0.152528493443
</pre>
<p>
<b>Funny:</b>: The test error is consistently smaller than the training error.<br>
<b>Answer:</b>: This seems to be a quirk of the data, when we train with the first part of
the data, we don't see that anymore.<br>
<p>
<b>Funny:</b>: Cannot get the network to overfit and see the test error go up.<br>
<p>
<b>Funny:</b>: The average error is -0.15, not near 0.
<p>
<img src="boats/mses_min.png"><img src="boats/diff_min.png">
<p>
Above left is the mean squared error for the training set (blue) and test set (red).<br>
Above right is the difference between the target points in the test set and net responses.<br>
<p>
<pre>
Found max dictionary:
{'width_meters': 5.5, 'draft_meters': 3.2, 'build_year': 2015.0, 'asking_price_euros': 912500.0, 'engine_build_year': 2015.0, 'length_over_all_meters': 23.0, 'displaces_kgs': 25000.0, 'ballast_kgs': 4000.0}
Found min dictionary:
{'width_meters': 1.62, 'draft_meters': 0.23, 'build_year': 1960.0, 'asking_price_euros': 1.0, 'engine_build_year': 1975.0, 'length_over_all_meters': 4.23, 'displaces_kgs': 100.0, 'ballast_kgs': 280.0}
</pre>

<h5 align=right>2016 mar 7</h5>
<h3>Understand why test error larger than train error</h3>
Answered above, still, we perform a thorough check:<br>
<ul>
  <li>Data properly separated? -- Yes, walked through the code.</li>
  <li>Outliers? -- Yes, removed some high-priced boats. Still not a good fit.</li>
  <li>Overfitting? -- No, even with 10000 neurons! <b>Why is this so?!</b></li>
</ul>

<h5 align=right>2016 mar 10</h5>
<h3>Overfitting reproducing</h3>
Could not reproduce the overfitting problem.
Made nice live plotting, one can see the network approaching a sine (from Hagan p. 11-23) but obviously with 9 neurons it interpolates fine.
<p>
We give it up, the net works nice, slow but nice.

<h3>Hagan Case Study I Revisited</h3>
Very nice results.

<h5 align=right>2016 mar 11</h5>
<h3>Why straight line ...</h3>
... for the hagan slow fit:
<pre>
[ytsboe@ytsboe-Latitude-E7250 ~/github/nn (master)]
$ dir apps/hagan_sinus_fit_slow.py 
-rw-rw-r-- 1 ytsboe ytsboe 2416 Mar 11 08:59 apps/hagan_sinus_fit_slow.py
</pre>
After initialisation, there is always a straight line ... why?
<p>
The inital weights are randomly chosen, verified.<br>
The response values are similar, but not the same.<br>
<p>
After setting the learning rate to 0.01, it worked beautifully!
<h3>Levenberg - Marquard</h3>
What the bloody hell is <b>v</b>(<b>x</b>) in eq. (13.32)?!
<p>
Can't reconsiliate the dimensions of <b>v</b>(<b>x</b>) with those of Delta_<b>x</b> ...

<h5 align=right>2016 mar 12</h5>
Previous problems fixed.
<p>
Now we obvserve the following:
<p>
After calculating the Jacobian and entering the iterative part to decrease the rms, we notice that the "peek" value for the rms converges exactly to the existing rms.<br>
Why does it not go below ... ?
<p>
If the Jacobian is properly built, then eventually the rms must drop, even if by so little.
<p>
Hm, currently having big trouble finding out which s^m_(i,h) in eq. (12.43)
maps to what element of S^m in eq. (12.48) ...
<p>
<h5 align=right>2016 mar 14</h5>
Fixed a bug and now we actually converge in little steps for Hagan case study I. :)
<p>
Still left to do: Walk again through the calculations of S and "get_mia", because with our current setup we have not been able to verify those calculations. We'd actually need data with multi dimensional output.
<p>

