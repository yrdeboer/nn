<a name="top"></a>
<a href="#bottom">Bottom</a>

<h5 align=right>2016 mar 5</h5>
<h2>Reproduce Hagan Example</h2>
Creating this in simple_backprop.py, using Hagan example on p. 11-14 to 11-17.
<p>
  Reproduced Hagan numbers.
<p>
  Refactored and created test case for simple two layer backprop using Hagan text.<br>
  Testing like:
  <pre>
    (nn_python2)
    [ytsboe@ytsboe-Latitude-E7250 ~/github/nn (master)]
    $ python ./test_simple_backprop.py
  </pre>
  <h2>Prepare data</h2>
  Fetched "raw" data from Digital Ocean site, put here: <tt>/home/ytsboe/data/boats/raw_data</tt>
<p>
  Script to convert raw boat data to computer reabable:
  <tt>/home/ytsboe/github/nn/boats/raw_to_computer_readable.py</tt>
<p>
  Here "computer readable" meamns ready for making plots etc., but before "cleaning" (removing outliers etc.)
<p>
  Tested fetching of data:
  <pre>
    Averages:
    length_over_all_meters: 11.5057620164
    width_meters: 3.61302763081
    build_year: 1993.74452986
    draft_meters: 1.75096601073
    displaces_kgs: 9198.89443652
    ballast_kgs: 3111.76237624
    engine_build_year: 1998.50480769

    Sums:
    None: 1005.0
    jeanneau: 122.0
    etap: 37.0
    contest: 37.0
    beneteau: 98.0
    hanse: 95.0
    hallberg-rassy: 73.0
    dehler: 60.0
    dufour: 60.0
    bavaria: 147.0
  </pre>
  All fine.

  <h2>Analyse/clean input data</h2>
  We'll have to add cuts to the data for sanitation.<br>
  For example:
  <ul>
    <li>Remove very long boat (93m): <tt>/home/ytsboe/data/boats/raw_data/santarelli-modulo-93-89194.dat</tt></li>
    <li>Remove very broad boat (32m): <tt>/home/ytsboe/data/boats/raw_data/j-boats-35-91943.dat</tt></li>
  </ul>
<p>
  The cuts will be:
<p>
  <table border=1>
    <tr><td>
        asking price
      </td><td>
        gt 1 and lt 1000000
    </td></tr>
    <tr><td>
        length
      </td><td>
        lt 30m
    </td></tr>
    <tr><td>
        width
      </td><td>
        lt 8m
    </td></tr>
    <tr><td>
        build year
      </td><td>
        gt 1960
    </td></tr>
    <tr><td>
        draft
      </td><td>
        lt 3.3
    </td></tr>
    <tr><td>
        displaces
      </td><td>
        lt 50000
    </td></tr>
    <tr><td>
        ballast
      </td><td>
        lt 8000
    </td></tr>
  </table>
  Stored the pdf with the plots here: <tt>/home/ytsboe/github/nn/boats/20160305_histograms_raw_1.pdf</tt>
<p>
  Applied these cuts, plots have now better range: <tt>/home/ytsboe/github/nn/boats/20160305_histograms_cleaned.pdf</tt>
<p>
  <h5 align=right>2016 mar 6</h5>
  Normalised the data, plots here:<br>
  <pre>
    [ytsboe@ytsboe-Latitude-E7250 ~/github/nn/boats (master)]
    $ cp feature_distributions.pdf 20160306_histograms_normalised.pdf
  </pre>
  Price correlates with length, with, draft and build year.<br>
  Little extra information expected from balast, displacement and engine build year.<br>
<p>
  Plotted features scatter plots:
  <pre>
    [ytsboe@ytsboe-Latitude-E7250 ~/github/nn/boats (master)]
    $ cp feature_scatter_plots.pdf 20160306_feature_scatter_plots.pdf 
  </pre>
  We see the expected large correlation of length with both with and draft.<br>
  Same for ballast and displacement, where also the subsitution of averages
  spoils the correlation.<br>
<p>
  In short, we expect the length and build year as the only significant features.<br>
<p>
  Plotted distributions of asking prices for all builders and compared the averages:
  <pre>
    $ python plot_input_data.py
    Asking prices (26.0, avg=-0.90) hist for builder: compromis
    Asking prices (810.0, avg=-0.79) hist for builder: None
    Asking prices (121.0, avg=-0.81) hist for builder: jeanneau
    Asking prices (37.0, avg=-0.88) hist for builder: etap
    Asking prices (34.0, avg=-0.76) hist for builder: contest
    Asking prices (24.0, avg=-0.87) hist for builder: victoire
    Asking prices (28.0, avg=-0.84) hist for builder: elan
    Asking prices (92.0, avg=-0.82) hist for builder: beneteau
    Asking prices (68.0, avg=-0.73) hist for builder: hallberg-rassy
    Asking prices (86.0, avg=-0.67) hist for builder: hanse
    Asking prices (60.0, avg=-0.83) hist for builder: dehler
    Asking prices (60.0, avg=-0.85) hist for builder: dufour
    Asking prices (146.0, avg=-0.83) hist for builder: bavaria
    Average average price = -0.812876360256

    [ytsboe@ytsboe-Latitude-E7250 ~/github/nn/boats (master)]
    $ cp asking_price_builder_names.pdf 20160306_asking_price_builder_names.pdf
  </pre>
  Hallberg Rassy and Hanse boats are significantly more expensive than average.<br>
  The Compromis boats are significantly less expensive.
<p>
  Managed to apply nn to the boats.<br>
  Found various minima, lowest about 0.03 for mse.<br>
<p>
  Below: error is plain error, not mse.
  <pre>
    $ python ./nn_simple_boats.py
    S1 = 5 alpha = 1e-05
    Iteration: 10 error train: [[ 0.75295619]] error test: [[ 0.77302896]]  
    Iteration: 14 error train: [[ 0.75286913]] error test: [[ 0.77293925]]
    ...
    Iteration: 4468866 error train: [[ 0.19906822]] error test: [[ 0.18165948]]
    Iteration: 6351925 error train: [[ 0.19906822]] error test: [[ 0.18165948]]
    Iteration: 9028455 error train: [[ 0.19906822]] error test: [[ 0.18165948]]
    Saving mses plot to mses.png

    W1    = [[-0.24212475  0.02694244  0.02821757 -0.08954417 -0.06884544 -0.22029359
    -0.17824722  0.12016396  0.07804248 -0.12401254 -0.23812842  0.0067572
    -0.07502254  0.09752009 -0.14860355 -0.22934456  0.04440873 -0.19319109
    -0.21739163  0.04663912]
    [-0.19111259 -0.04972988  0.2766238  -0.20549939 -0.23706564 -0.00905296
    -0.07868101 -0.01806229  0.11763035  0.21292985 -0.06923144  0.03853001
    -0.18160576  0.15787528 -0.03442326 -0.14595591  0.06767789  0.15274417
    -0.23525094  0.01755163]
    [ 0.08318391 -0.18704918  0.02575674 -0.21834164 -0.02657485 -0.03083301
    -0.01879369 -0.13354374 -0.07215475  0.10100178  0.01558534 -0.20290369
    -0.02158654  0.10643996 -0.17490313  0.01301879  0.15909314  0.23171723
    -0.0187789   0.05825458]
    [ 0.17224199  0.25418321  0.17877259  0.00282     0.10864183  0.14682755
    -0.18455865 -0.03242237  0.10119335  0.01979395 -0.16845622  0.10966697
    0.0434327   0.12205877  0.22940673  0.21840603 -0.13924256 -0.04318723
    -0.24359756  0.07975306]
    [ 0.25373977  0.09699836 -0.25198931  0.04910205 -0.12344794  0.05795805
    0.00623658 -0.03152442 -0.01956241 -0.08682047  0.08043537 -0.14919747
    -0.05816138 -0.18726213  0.08601861  0.12166136 -0.0105374  -0.035562
    -0.1693814   0.03321968]]
    b1vec = [[-0.03134725]
    [ 0.15939111]
    [-0.18518457]
    [-0.00446784]
    [-0.11154539]]
    W2    = [[-0.23789441 -0.41403788  0.09546187  0.23704932  0.22342746]]
    b2vec = [[-0.39715867]]
    Average diff = -0.150578919606 sd = 0.152528493443
  </pre>
<p>
  <b>Funny:</b>: The test error is consistently smaller than the training error.<br>
  <b>Answer:</b>: This seems to be a quirk of the data, when we train with the first part of
  the data, we don't see that anymore.<br>
<p>
  <b>Funny:</b>: Cannot get the network to overfit and see the test error go up.<br>
<p>
  <b>Funny:</b>: The average error is -0.15, not near 0.
<p>
  <img src="boats/mses_min.png"><img src="boats/diff_min.png">
<p>
  Above left is the mean squared error for the training set (blue) and test set (red).<br>
  Above right is the difference between the target points in the test set and net responses.<br>
<p>
  <pre>
    Found max dictionary:
    {'width_meters': 5.5, 'draft_meters': 3.2, 'build_year': 2015.0, 'asking_price_euros': 912500.0, 'engine_build_year': 2015.0, 'length_over_all_meters': 23.0, 'displaces_kgs': 25000.0, 'ballast_kgs': 4000.0}
    Found min dictionary:
    {'width_meters': 1.62, 'draft_meters': 0.23, 'build_year': 1960.0, 'asking_price_euros': 1.0, 'engine_build_year': 1975.0, 'length_over_all_meters': 4.23, 'displaces_kgs': 100.0, 'ballast_kgs': 280.0}
  </pre>

  <h5 align=right>2016 mar 7</h5>
  <h3>Understand why test error larger than train error</h3>
  Answered above, still, we perform a thorough check:<br>
  <ul>
    <li>Data properly separated? -- Yes, walked through the code.</li>
    <li>Outliers? -- Yes, removed some high-priced boats. Still not a good fit.</li>
    <li>Overfitting? -- No, even with 10000 neurons! <b>Why is this so?!</b></li>
  </ul>

  <h5 align=right>2016 mar 10</h5>
  <h3>Overfitting reproducing</h3>
  Could not reproduce the overfitting problem.
  Made nice live plotting, one can see the network approaching a sine (from Hagan p. 11-23) but obviously with 9 neurons it interpolates fine.
<p>
  We give it up, the net works nice, slow but nice.

  <h3>Hagan Case Study I Revisited</h3>
  Very nice results.

  <h5 align=right>2016 mar 11</h5>
  <h3>Why straight line ...</h3>
  ... for the hagan slow fit:
  <pre>
    [ytsboe@ytsboe-Latitude-E7250 ~/github/nn (master)]
    $ dir apps/hagan_sinus_fit_slow.py 
    -rw-rw-r-- 1 ytsboe ytsboe 2416 Mar 11 08:59 apps/hagan_sinus_fit_slow.py
  </pre>
  After initialisation, there is always a straight line ... why?
<p>
  The inital weights are randomly chosen, verified.<br>
  The response values are similar, but not the same.<br>
<p>
  After setting the learning rate to 0.01, it worked beautifully!
  <h3>Levenberg - Marquard</h3>
  What the bloody hell is <b>v</b>(<b>x</b>) in eq. (13.32)?!
<p>
  Can't reconsiliate the dimensions of <b>v</b>(<b>x</b>) with those of Delta_<b>x</b> ...

  <h5 align=right>2016 mar 12</h5>
  Previous problems fixed.
<p>
  Now we obvserve the following:
<p>
  After calculating the Jacobian and entering the iterative part to decrease the rms, we notice that the "peek" value for the rms converges exactly to the existing rms.<br>
  Why does it not go below ... ?
<p>
  If the Jacobian is properly built, then eventually the rms must drop, even if by so little.
<p>
  Hm, currently having big trouble finding out which s^m_(i,h) in eq. (12.43)
  maps to what element of S^m in eq. (12.48) ...
<p>
  <h5 align=right>2016 mar 14</h5>
  Fixed a bug and now we actually converge in little steps for Hagan case study I. :)
<p>
  Still left to do: Walk again through the calculations of S and "get_mia", because with our current setup we have not been able to verify those calculations. We'd actually need data with multi dimensional output.
<p>

  <h5 align=right>2016 mar 18</h5>
  <h2>Moved to conda/ python3 on Lenovo y700</h2>
  Had to alter the plotting routine, now it actually requires a pause after calling show, to allow for plotting :(
<p>
  Created training data to learn binary counting, with R=1, S1=2, S2=3 and Q=8.<br>
  The purpose was to check the calculations for the Jacobian for layer 2.<br>
  We made some changes, but now the hagan sinus does not work anymore.
<p>
  We made the following changes:
  <ul>
    <li>Aaarrrgghh .. found it, it was an indent in the rms calculation. All fine again!
    </li>
  </ul>
<p>
  Now trying to understand why I don't seem to be able to teach the 1-N-3 network to express the numbers 1 to 8 in binary format ...<br>
  <pre>python apps/binary_simple_test_levenberg_marquard.py</pre>
<p>
  Brainstorming options:
  <table border=1>
    <tr>
      <td><b>Option</b></td><td><b>Possible approach or solution</b></td>
    </tr>
    <tr>
      <td>Error in Jacobian</td>
      <td>
        1. Work through binary problem again<br>
        2. Find numerical code to verify<br>
        3. ...
      </td>
    </tr>
    <tr>
      <td>Network cannot learn binary</td>
      <td>
        1. Can I understand why?<br>
        2. <b>Maybe try with 3-N-3 network (like in Mitchell fig. 4.7)</b><br>
      </td>
    </tr>
  </table>
<p>
  Tried with binary input and it work beautifully!
<p>
  So it was not possible to learn the network to count binary ... have to think why some more, but I now trust the LM algoritm! :)
<p>
  Made three tests:
  <pre>
(venv3)ytsen @ y700 ~/github/nn (master)
└─ $ ▶ for i in $( ls apps/test*.py ) ; do python $i ; done
SUCCESS (Levenberg-Marquard Hagan Case study 1 -- light sensors)
SUCCESS (Levenberg-Marquard ("binary"))
SUCCESS (Levenberg-Marquard Jacobian, Hagan)
SUCCESS (Hagan 2-layer simple backprop)
  </pre>

  <h5 align=right>2016 mar 21</h5>

  <h3>Starting with Bayesian regularisation</h3>
  Working in file:
  <pre>
    ~/github/nn/nets/levenberg_marquard_backprop_bay_reg.py
  </pre>
  with a working file for the Levenberg-Marquard algo:
  <pre>
    ~/github/nn/apps/overfitting_lb_with_and_without_bay_reg.py
  </pre>

  Problem: Step 1 in the algo according to Hagan p. 13-16:
  <pre>
    Take 1 step of the Levenberg-Marquard algorithm toward minimising
    the objective function F(x) = beta * E_d + alpha * E_w
  </pre>
  <b>But</b>: In our code, the Jacobian is calculated using the error. How
  does the Jacobian differ when using Levenberg-Marquard ... ?
  
  <h5 align=right>2016 mar 23</h5>
  <h3>Bayesian Regularisation does not work</h3>
  Implemented it seemingly fine, but we cannot get it to visibly work.
<p>
  It seemed to work, using a "wrong" formula, which worked for the slow sinus,
  but not for the Hagan case study I.
<p>
  Cannot reproduce the figure 13.7 in Hagan, our code just continues to overfit ... ?
<p>
  <h5 align=right>2016 mar 24</h5>
  With fixed regularisation, we are able to reproduce the plots of fig. 13.6 (Hagan) p. 13-10.
<p>
  Something is wrong with the Bayesian regularisation, when the expectation values stablise,
  then F(x) converges to a fixed number, what would the stopping criterium be if F(x) does not
  change ... ?
<p>
  Still no luck, found some papers about the determining alpha and beta, but they formulate
  the procedure no more detailed than Hagan.
<p>
  <h3>Why do alpha and beta not stabilise?!</h3>

  <h5 align=right>2016 mar 25</h5>
  
  Posted this question on StackExchange: <a href="http://stats.stackexchange.com/questions/203645/bayesian-regularisation-for-anns-why-does-the-iterative-algo-work">http://stats.stackexchange.com/questions/203645/bayesian-regularisation-for-anns-why-does-the-iterative-algo-work</a>.
<p>
  It occurred to me that I take the Jacobian as per Hagan eq. (12.37).
<p>
  Its elements are derivatives to the weights of v's, where the v's are the squared items in the objective function <i>F(x) = Sum (v)^2</i>, where the v's are the differences between the training target data and the network response.
<p>
  However, With the new objective function <i>F(x) = beta*Ed + alpha*Ew</i>, we have extra v's, namely: <i>sqrt(alpha/beta)*wgt_i</i>!
<p>
  And the derivatives to the weights are easy to compute, no backpropagation needed!
<p>
  <h5 align=right>2016 mar 26</h5>
  Consolidated the non-working regularisation stuff in <tt>~/github/nn/nets/levenberg_marquard_backprop_bay_reg.py</tt>.
<p>
  Checked the test (<tt>test_levenberg_marquard_binary</tt>) still works for the regularised algo.
<p>
  Switching back to default Levenberg-Marquard <tt>~/github/nn/nets/levenberg_marquard_backprop.py</tt> and going to implement early stopping for the Hagan sinus case in: <tt>~/github/nn/apps/hagan_sinus_fit_slow_early_stopping.py</tt>, before applying it to the boats.
<p>
  <h3>Uh?! Why is new version so much faster</h3>
  Observe that <tt>nets/levenberg_marquard_backprop_bay_reg.py</tt> is much faster than <tt>nets/levenberg_marquard_backprop.py</tt>, even though both pass the Levenberg-Marquard tests ...
<p>
  Got it, had some extra unnecessary copying and transposition of the jacobian.
<p>
  <h5 align=right>2016 mar 27</h5>
  <h3>On to Levenberg-Marquard for the boats</h3>
  Working in:
  <pre>
    (venv3)ytsen @ y700 ~/github/nn (master)
    └─ $ ▶ cp boats/nn_simple_boats.py boats/nn_boats_lb_early_stopping.py 
    (venv3)ytsen @ y700 ~/github/nn (master)
  </pre>

  Typical rms value on the validation/test set ~0.16, reached after less than 10 iterations.
<p>
  Asking price seems to give better response when take log, since the peak is now
  not on a side and to both sides the histogram diminishes.
<p>
  25-40 nodes in the hidden layer seem adequate, gets the validation error down to 0.11.
<p>
  Running again for same validation rms and plotting the test y-y^hat distribution:
<p>
  <img src="plots/nn_boats_lb_early_stopping.png">(Used 40 nodes in the hidden layer).
<p>
  <h5 align=right>2016 mar 28</h5>
  Implemented comparision with OLS, ready to implement the network into the site.
<p>
  <h3>Set up site</h3>
  Working locally in new github repo (yrdeboer/boats_estimator):
  <pre>
    (venv3)ytsen @ y700 ~/github/boats_estimator (master)
    $ git remote add origin git@github.com:yrdeboer/boats_estimator.git
  </pre>

  <h5 align=right>2016 mar 29</h5>
  <h3>Reproduce net response with saved weights</h3>
  Checked plots, now able to reproduce a network from saved data:
  <pre>
(venv3)ytsen @ y700 ~/github/nn (master)
└─ $ ▶ python boats/reproduce_net_from_data.py    
  </pre>

  <h5 align=right>2016 mar 31</h5>
  <h3>Sensitivity Analysis</h3>
  Added sensitivy tracking, ordered the parameters:
  <pre>
    ipdb> pprint.pprint(sorted(tuple(zip(dfdp_lst, features))))
[(-1033.0096877708813, 'length_over_all_meters'),
 (-362.5028003600454, 'build_year'),
 (-357.5900789182798, 'hallberg-rassy'),
 (-251.92739075046998, 'victoire'),
 (-212.4983765121567, 'contest'),
 (-191.9251843628137, 'None'),
 (-165.47068121158213, 'compromis'),
 (-165.2635180274413, 'dehler'),
 (-150.16300550824178, 'etap'),
 (-102.12385979304005, 'elan'),
 (-91.8584283029938, 'width_meters'),
 (-87.76366641673947, 'jeanneau'),
 (-80.72036168843829, 'engine_build_year'),
 (-71.69833130723515, 'beneteau'),
 (-56.901554160828105, 'dufour'),
 (-51.8255477116102, 'hanse'),
 (-29.138273549904973, 'bavaria'),
 (3.1056364205509794, 'ballast_kgs'),
 (11.157138496882078, 'displaces_kgs'),
    (83.39356546481153, 'draft_meters')]
  </pre>
  The weight values are taken at minimum rms for the validation set.
  The derivatives are taken on the training set.
<p>
  These values are the summed derivatives of the sum squared error
  to the input vectors, as per Hagan eq. (17.25):

  <pre>
    Sensitivities for iteration 29
[(-1353.54937921491, 'length_over_all_meters'),
 (-413.8675449561241, 'hallberg-rassy'),
 (-399.7505157930764, 'compromis'),
 (-376.8573966481697, 'build_year'),
 (-320.94576351303317, 'victoire'),
 (-113.19930294182328, 'contest'),
 (-99.79940474631616, 'etap'),
 (-97.46511528409533, 'None'),
 (-57.68741177750272, 'dehler'),
 (-16.88788289038877, 'beneteau'),
 (4.9571181650697875, 'jeanneau'),
 (5.617108842392814, 'elan'),
 (44.07794716376523, 'hanse'),
 (53.59719742379652, 'bavaria'),
 (70.1406400148975, 'dufour')]
</pre>

  How much does this say about the most important input parameters ... ?
<p>
  We should do a variance/covariance analysis ...
  









  
  <a name="bottom"></a>
  <a href="#top">Top</a>
